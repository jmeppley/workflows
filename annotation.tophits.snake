"""
A makefile to find the best hits of s aset of sequences in a given database

Given:

 * A set of sequences files (eg. {sample}/reads.fastq for N samples)

Do the following for each set of sequences:

 * search against a sequence DB (EG RefSeq or a gene catalog)
 * assign a single top hit to each sequence

Finally
 * compile a table of hit counts by sample

"""
import re, pandas
from python.common import get_version, apply_defaults, get_file_name
from python.tophit import get_top_hit_outputs, get_non_rna_reads, get_get_ids_cmd
from python.annotate import get_db_ids_file
from snakemake.logging import logger

defaults = {'filter': {'F': 0, 'B': 50}} 
apply_defaults(config, defaults)

needs_qc = get_top_hit_outputs(config)
if needs_qc:
    include: "qc/setup.snake"

outputs = config.setdefault('outputs',set())
logger.debug(config)

include: 'common/stats.snake'
include: "annotation/lastal.snake"
include: "annotation/bwa.snake"
include: "common/fastq.snake"
include: "common/transitions.snake"

if config.get('remove_rna', True) in ['True', True]:
    include: "qc/sort.rna.snake"
    ruleorder: filter_nonrrna_or_rrna > fastq_to_fasta

wildcard_constraints:
    search=r'vs\..+',
    alg=r'(last[xnp]|blast[xnp]|sam)',
    top=r'(tophit|toporg)'

rule all:
    input: outputs

filter_params_patt=r'(?:_([a-zA-Z])(-?[0-9.]+))'
filter_params_rexp=re.compile(filter_params_patt)
rule filter_hits:
    input: '{file_root}.{alg}'
    output: '{file_root}.{alg}.{params}'
    wildcard_constraints:
        params=filter_params_patt + r'+'
    params:
        opts=lambda w: \
            " ".join(["-{} {}".format(o,v) \
                      for o,v in filter_params_rexp.findall(w.params)]),
        fmt=lambda w: 'blast' if re.search('last', w.alg) else w.alg,
    benchmark:
        'benchmarks/{file_root}.{alg}.filter.{params}.time'
    version:
        get_version('filter_blast_m8.py')
    shell:
        "filter_blast_m8.py {params.opts} -f {params.fmt} -o {output} {input}"

rule assign_top_hits:
    input:
        expand('{sample}.vs.{{db}}.{{alg}}.{{filter}}',
               sample=config['sample_data'])
    output:
        expand('{sample}.vs.{{db}}.{{alg}}.{{filter}}.{{top}}',
               sample=config['sample_data'])
    benchmark:
        'benchmarks/assign_top_hits.{db}.{alg}.{filter}.{top}.time'
    version:
        get_version('assign_top_hit.py')
    params:
        fmt=lambda w: 'blast' if re.search('last', w.alg) else w.alg,
        # assign_top_hit will interperet -o as a suffix if >1 file given
        output=lambda w: '.' + w.top if len(config['sample_data']) > 1 \
                   else ' '.join(
                            expand('{sample}.vs.{db}.{alg}.{filter}.{top}', \
                                   db=w.db, alg=w.alg, top=w.top, \
                                   filter=w.filter, \
                                   sample=config['sample_data'])) 
    shell:
        "assign_top_hit.py -f {params.fmt} -C {wildcards.top} \
        -o {params.output} {input}"

rule remove_rRNA_hits:
    """ remove rRNA reads from count table (only works for fastq input)
    We do this post search, so the RNA filtering can happen in parallel
    with the database search. (cmsearch is slow compared to last & bwa)
    
    If your data has a high percentage rRNA reads, consider filtering
    before starting this workflow (and using remove_rna=False).
    """

    input:
        hits='{sample}.{search}.{top}',
        nonrna=lambda w: get_non_rna_reads(w, config)
    output: '{sample}.{search}.{top}.non-rRNA'
    threads: 2
    benchmark: 'benchmarks/{sample}.{search}.{top}.non-rRNA.time'
    params:
        get_ids_cmd=lambda w: get_get_ids_cmd(w, config)
    shell: "screen_table.py {input.hits} -k \
             -l <({params.get_ids_cmd}) > {output}"

rule compile_hit_counts:
    """ take hit tables and count hits to each hitid 
    
    if {rna} in the output file is non-rRNA,
        use the ".non-rRNA" version of the top hits
    if {rna} in the output file is "all":
        use the full top hits file (no suffix)

    Uses count_taxa.py to count filtered hit tables
    """
    input:
        lambda w: expand('{sample}.{search}.{top}{rna}',
                         sample=config['sample_data'],
                         search=config['db_strings'][w.db],
                         top=w.top,
                         rna="" if w.rna == 'all' else '.' + w.rna)
    output:
        'counts.{db}.{top}.{rna}.hitids'
    params:
        input=lambda w: ' '.join('{s}={s}.{search}.{top}{rna}'.format(
                                 s=sample,
                                 search=config['db_strings'][w.db],
                                 top=w.top,
                                 rna="" if w.rna == 'all' else '.' + w.rna) \
                             for sample in config['sample_data']),
        fmt=lambda w: 'blast' \
                        if re.search('last', config['dbs'][w.db]['format']) \
                        else 'sam',
    version:
        get_version("count_taxa.py")
    log:
        'logs/counts.{db}.{top}.{rna}.hitids.log'
    benchmark:
        'benchmarks/counts.{db}.{top}.{rna}.hitidis.time'
    shell:
        'count_taxa.py {params.input} -f {params.fmt} -v > {output} 2> {log}'

rule translate_and_collapse_counts:
    input:
        counts='counts.{db}.{top}.{rna}.hitids',
        id_map=lambda w: get_db_ids_file(w.db, config)
    output: 'counts.{db}.{top}.{rna}.translated'
    benchmark: 'benchmarks/counts.{db}.{top}.{rna}.translate.time'
    run:
        counts = pandas.read_csv(get_file_name(input.counts),
                                 sep='\t',
                                 header=0, index_col=0)

        hit_id_map = {}
        with open(input.id_map) as IDS:
            for line in IDS:
                hitid, description = line.split(None, 1)
                if hitid in counts.index:
                    hit_id_map[hitid] = description.strip()

        counts['descriptions'] = [hit_id_map.get(h, h) for h in counts.index]

        counts.groupby('descriptions').agg(sum).to_csv(get_file_name(output),
                                                       sep='\t')


