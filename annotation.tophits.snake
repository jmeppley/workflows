"""
A makefile to find the best hits of s aset of sequences in a given database

Given:

 * A set of sequences files (eg. {sample}/reads.fastq for N samples)

Do the following for each set of sequences:

 * search against a sequence DB (EG RefSeq or a gene catalog)
 * assign a single top hit to each sequence

Finally
 * compile a table of hit counts by sample

"""
import re
from python.common import get_version
from python.tophit import get_top_hit_outputs, get_non_rna_reads, get_get_ids_cmd
from snakemake.logging import logger

needs_qc = get_top_hit_outputs(config)
if needs_qc:
    include: "qc/setup.snake"

outputs = config.setdefault('outputs',set())
logger.debug(config)

include: 'common/stats.snake'

include: "annotation/lastal.snake"
include: "common/fastq.snake"
include: "common/transitions.snake"

if config.get('remove_rna', True) in ['True', True]:
    include: "qc/sort.rna.snake"
    ruleorder: filter_nonrrna_or_rrna > fastq_to_fasta

wildcard_constraints:
    search=r'vs\..+',
    alg=r'(last[xnp]|blast[xnp]|sam)',
    top=r'(tophit|toporg)'

rule all:
    input: outputs

filter_params_patt=r'(?:_([a-zA-Z])(-?[0-9.]+))'
filter_params_rexp=re.compile(filter_params_patt)
rule filter_m8:
    input: '{file_root}.{alg}'
    output: '{file_root}.{alg}.{params}'
    wildcard_constraints:
        params=filter_params_patt + r'+'
    params:
        opts=lambda w: \
            " ".join(["-{} {}".format(o,v) \
                      for o,v in filter_params_rexp.findall(w.params)]),
        fmt=lambda w: 'blast' if re.search('last', w.alg) else w.alg,
    benchmark:
        'benchmarks/{file_root}.{alg}.filter.{params}.time'
    version:
        get_version('filter_blast_m8.py')
    shell:
        "filter_blast_m8.py {params.opts} -f {params.fmt} -o {output} {input}"

rule assign_top_hits:
    input:
        expand('{sample}.vs.{{db}}.{{alg}}.{{filter}}',
               sample=config['sample_data'])
    output:
        expand('{sample}.vs.{{db}}.{{alg}}.{{filter}}.{{top}}',
               sample=config['sample_data'])
    benchmark:
        'benchmarks/assign_top_hits.{db}.{alg}.{filter}.{top}.time'
    version:
        get_version('assign_top_hit.py')
    params:
        fmt=lambda w: 'blast' if re.search('last', w.alg) else w.alg,
    shell:
        "assign_top_hit.py -f {params.fmt} -C {wildcards.top} \
        -o .{wildcards.top} {input}"

rule count_hits:
    """ take a hit table and count hits to each hitid """
    input:
        '{sample}.{search}.{top}{rna}'
    output:
        '{sample}.{search}.{top}{rna}.hitid.counts'
    benchmark:
        "benchmarks/{sample}.{search}.{top}.count.time"
    log:
        "logs/{sample}.{search}.{top}{rna}.count.log"
    version:
        get_version("count_hits.py")
    shell:
        "count_hits.py -v -i {input} -H 1 \
        > {output} 2> {log}"

rule filter_hits:
    """ remove rRNA reads from count table (only works for fastq input)"""
    input:
        hits='{sample}.{search}.{top}',
        nonrna=lambda w: get_non_rna_reads(w, config)
    output: '{sample}.{search}.{top}.non-rRNA'
    threads: 2
    benchmark: 'benchmarks/{sample}.{search}.{top}.non-rRNA.time'
    params:
        get_ids_cmd=lambda w: get_get_ids_cmd(w, config)
    shell: "screen_table.py {input.hits} -k \
             -l <({params.get_ids_cmd}) > {output}"

rule compile_hit_counts:
    """
    Uses join utility to merge hit count tables

    We use some magice to get an "outer" join:

    * the -a options force joint to output all lines of all files
    * the -o option fills in columsn from other files in unjoined lines
    """
    input:
        lambda w: expand('{sample}.{filter}.{top}.{rna}hitid.counts',
                         filter=config['db_strings'][w.db],
                         top=w.top,
                         rna=w.rna,
                         #rna="non-rRNA." \
                         #    if config.get('remove_rna', True) in ['True', True] \
                         #    else "",
                         sample=config['sample_data'])
    output:
        'counts.{db}.{top}.{rna}hitids'
    params:
        input_map=lambda w: {s:'{sample}.{filter}.{top}.hitid.counts'.format(
         
         sample=s,
                                top=w.top,
                                filter=config['db_strings'][w.db]) \
                             for s in config['sample_data']},
        #dash_as=' '.join('-a {}'.format(i) \
        #                 for i in range(len(config['sample_data']))),
        #fields=','.join("{}.1".format(i) \
        #                for i in range(len(config['sample_data']))),
        #keys=' '.join("-{} 2".format(i) \
        #              for i in range(len(config['sample_data']))),
    benchmark:
        'benchmarks/counts.{db}.{top}.hitidis.time'
    run:
        import pandas, re
        for sample, input_file in params.input_map.items():
            sample_counts = pandas.read_table(input_file, 
                                              index_col=0,
                                              names=[sample,])
            try:
                counts = counts.join(sample_counts, how='outer')
            except NameError:
                counts = sample_counts
        counts.to_csv(output[0], sep='\t')





###TODO: how can we assign top hit accross all samples, yet leave the separate
#files???
# * assign_tophits.py will take multiple files. We need to modify the
#   rule to take advantage of this...
