"""
Python Makefile to pull out reads from a BAM alignment 
  and use RefSeq, KEGG, PFAM, and others to annotate them
Annotations are generated as tables of gene_family counts 
  groupbed by taxonomic clade

Supports:
 * bam, fasta, or fastq inputs
 * .faa (protein) inputs (will skip aa_conversion)
 * multiple input files (can't mix prot and nucl)

To run:

snakemake -s /path/to/workflows/annotate.reads.snake --configfile annotate.reads.yaml [OPTIONS]

See test/conda/annotate.yml for a list of rquired packages. You can create a conda environment with the command:
conda env create -n annotate -f test/conda/annotate.yml

You will need to run the makefiles in tools/pymg/databases two download and format the 
reference databases if your lab doesn't already have them.

With a config file that looks like test/data/configs/reads.annot.prod/yml or reads.annot.six.yml.

Or like this:
reads_file: /path/to/file.bam
annot_prefix: reads
clade_ranks:
    - order
    - family
    - genus
output_style: long
hmmer:
    threads: 3
lastal:
    threads: 10
prodigal:
    threads: 20
aa_conversion: prodigal
dbs:
    RefSeq:
        path: "/seqdbs/RefSeq/78/RefSeq-78.AllProteins.faa.ldb/lastdb"
        format: lastp
        type: tax
    KEGG:
        path: "/seqdbs/KEGG/KeggGene.pep.20160919/lastdb"
        format: lastp
        assign_type: kegg
    PFAM:
        path: "/projectb/scratch/jmeppley/seqdbs/PFAM/30.0/frag-20/Pfam-A.{N:02d}.hmm"
        frags: 10
    COG:
        path: "/projectb/scratch/jmeppley/seqdbs/cdd/16_January_2015/COG/frag-10/COG.hmm.{N:02d}.ascii"
        frags: 10

The reads can be embedded in a bam file or given as fasta or fastq.

Instead of supplying a reads file and annotation prefix, you can configure a
map from sample names to reads files:

sample_data:
    sample1: /path/to/sample1.fastq
    sample2: /path/to/sample2.fastq

Or, if your files are named in a consistent fashion, you can use a
wildcard_glob string:

sample_glob: /path/to/{sample}.fastq

Both the sample_data and sample_glob approaches use dynamic_remote_snake to
check for URLs. (https://github.com/jmeppley/dynamic_remote_snake). So you can
specify something like:

sample_glob: HTTP://server.domain.org/public/reads/{sample}.fastq

or

sample_glob: SFTP://server.domain.org/path/to/reads/{sample}.fastq
remote:
    SFTP:
        defaults:
            username: username

(NOTE: SCP/SFTP doesn't support passwords, so use SSH keys for the connection)
"""

import yaml
import os
from python.common import get_version
from python.annotate import get_db_dot_fmt_strings, get_last_alg
from python.mapping import check_sample_data
from jme.drs import remote_wrapper

#########
# CONFIGURATION
#
# defaults for basic operation
# ranks to collect taxa on (defaults to order)
config.setdefault('clade_ranks', ['order'])
#
# EXPERIMENTAL: support for glob_wildcards or detailed sample:fastq config map
#  checks for sample_data map or sample_glob template string
#    - sample_glob must have a "{sample}" wild card in it
#    - all files are run through remote_wrapper
if 'sample_data' in config or 'sample_glob' in config:
    check_sample_data(config)
sample_data = config.get('sample_data', {})

# read file names for start of workflow
#  old way: single file (use defaults if nothing else configured)
if 'annot_prefix' in config or 'reads_file' in config or len(sample_data) == 0:
    prefix = config.setdefault('annot_prefix', 'reads')
    reads_file = remote_wrapper( \
        config.setdefault('reads_file', '{prefix}.fasta'.format(**vars())),
        config)
    sample_data[prefix] = reads_file


## process sample_data file names
transitions = config.setdefault('transitions',{})
need_fastq = False
need_bam = False
faa_count = 0
for sample, reads_file in sample_data.items():
    #
    # If the starting file doesn't fit our naming scheme,
    #  call for a symbolic link to be made (using transitions dict)
    extension = re.search(r'\.([a-z0-1A-Z]+)$', reads_file).group(1)

    # workflow expects .fasta, so rename .fna, .faa, etc
    if extension in ['fna', 'ffn', 'fa', 'fasta']:
        extension = 'fasta'
    elif extension == 'faa':
        faa_count += 1
    if extension in ['fasta', 'fastq', 'bam', 'faa']:
        if extension == 'fastq':
            need_fastq = True
        elif extension == 'bam':
            need_bam = True
        annot_start_file = "{sample}.{extension}".format(**vars())
        if reads_file != annot_start_file:
            transitions[annot_start_file] = reads_file
    else:
        raise Exception("Initial file must be bam or fasta!" \
                        "I don't know what to do " \
                        "with {reads_file}".format(**vars()))
#
# End configuration
##########

include: "common/stats.snake"
aa_conversion = config.get('aa_conversion', 'sixframe')
if faa_count > 0 and faa_count < len(sample_data):
    raise Exception("If any input files are protein (.faa), they all must be")
if faa_count == len(sample_data):
    include: "annotation/common.genes.snake"
    include: "annotation/dummy.coverage.snake"
    stats_suffixes = ["faa",]
    fastq_query_extension = 'faa'
    if config.setdefault('remove_rna', False) in ['True', True]:
        raise Exception("Cannot run faa files through rRNA search. "
                        "Please set remove_rna: False")
elif aa_conversion == 'prodigal':
    include: "annotation/common.prodigal.snake"
    stats_suffixes = ['fasta', "pred.genes.faa"]
    fastq_query_extension = 'faa'
else:
    include: "annotation/common.sixframe.snake"
    stats_suffixes = ['fasta', "sixframe.faa"]
    fastq_query_extension = 'fasta'
include: "common/transitions.snake"

if need_fastq:
    include: "common/fastq.snake"
    if aa_conversion != 'prodigal':
        ruleorder: remove_colons > fastq_to_fasta
if need_bam and aa_conversion == 'prodigal':
    ruleorder: predict_genes_prodigal > extract_reads

if config.get('remove_rna',True) in ['True', True]:
    include: "qc/sort.rna.snake"
    rna_step = 'non-rRNA.'
    if need_fastq:
        ruleorder: filter_nonrrna_or_rrna > fastq_to_fasta
else:
    rna_step = ''

# set up list of files to create
#  - The final tabulations
#  - Stats and histrogram files for reads and translated seqs
gene_family_db_dot_fmts = get_db_dot_fmt_strings(gene_family_dbs,
                                                 config,
                                                 fastq_query_extension)

annotation_file_list = []
for taxdb in config['taxdbs']:
    taxalg = get_last_alg(config['dbs'][taxdb].get('format','lastp'),
                          fastq_query_extension)
    annotation_file_list.extend(
        expand("{prefix}.{rna_step}annot.{taxdb}.{taxalg}.{clade_rank}.vs.{db_dot_fmt}.tsv",
               prefix=sample_data,
               taxalg=taxalg,
               rna_step=rna_step,
               taxdb=taxdb,
               clade_rank=config['clade_ranks'],
               db_dot_fmt=gene_family_db_dot_fmts,))

annotation_file_list.extend(
            expand("stats/{prefix}.{rna_step}{suffix}.{ext}", 
                   ext=['stats','hist'],
                   suffix=stats_suffixes,
                   rna_step=rna_step,
                   prefix=sample_data))

logger.debug("Snakefile config:\n" + yaml.dump(config))
logger.debug(repr(annotation_file_list))

##########
# RULES:
#  This lays out the dependencies and logic of the workflow
#  After the "all" target, it is generally laid out start to finish
rule read_annotation_all:
    input:
        annotation_file_list

if extension == 'bam':
    rule extract_reads:
        """
        Pull reads out of BAM file
        """
        input:
            "{prefix}.bam"
        output:
            temp("{prefix}.fasta")
        benchmark:
            "benchmarks/{prefix}_extract_reads.time"
        log:
            "logs/{prefix}_extract_reads.log"
        version:
            get_version('samtools', lines=[0,])
        resources:
            disk=1
        shell:
            "samtools fasta -0 {output} {input} > {log} 2>&1"

