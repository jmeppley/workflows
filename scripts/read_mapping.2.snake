"""
read_mapping.2.snake

Calculate abundances by mapping reads to a reference catalog

 * Map reads to catalog with BWA or similar
 * Convert to sorted compressed BAM with only top hits to save space
 * Count hits in table

Changes from read_mapping.snake:
 
 * Doesn't use py-metagenomics scripts
 * uses BAM to save space instaed of dropping header
 * doesn't quite work!!!

There are two key configuration values:

  references_file: location of the reference database
  sample_glob: snakemake wildcard glob string to locate read files and name samples

The sample glob should look like:

    /path/to/read/files/{sample}.reads.fastq

Or even:

    SFTP://server.provider.com/remote_path/{project}/{sample}/reads.fastq

Just make sure that one of the wildcards is called 'sample'.


Files (including the glob) can be local or remote. Remote files should be formatted for the snakemake remote provider. EG

   SFTP://server.domain.edu/path/to/file.ext

Supported protocols are SFTP, HTTP, and FTP

login credentials should be passed in the configuration EG:

config = {'remote': {'FTP': {'server.domain.edu': {
    'username': 
    'password':
}

SFTP only works with keys (no password).

"""

# get remote file tools
import os, sys, pandas
workflows_dir = os.path.dirname(os.path.dirname(workflow.snakefile))
sys.path.append(workflows_dir)
from python.common import get_file_name
from python.mapping import check_sample_data

from jme.dynamic_remote_snake.remote import remote_wrapper, get_dl_snakefile
include: get_dl_snakefile()

references_file = remote_wrapper(config['references_file'], config)
logger.debug("references_file: " + repr(references_file))

search_result_template = "{sample}.vs.references.sam"
read_count_template = "{sample}.vs.references.read_counts"

check_sample_data(config)

search_results = {s:read_count_template.format(sample=s) \
                  for s in config['sample_data']}

rule outputs:
    input:
        "reference.hit_counts.tsv",

rule count_table:
    input: search_results.values()
    output: 'reference.hit_counts.tsv'
    benchmark: "benchmarks/count_table.time"
    log: "logs/count_table.log"
    params:
        hit_tables=' '.join(["=".join(kv) for kv in search_results.items()]),
        cutoff=config.get('cutoff', 0)
    run:
        merged_counts = {}
        for sample, count_file in search_results.items():
            sample_counts = merged_counts.setdefault(sample, {})
            with open(count_file) as count_handle:
                for line in count_handle:
                    count, ref = line.strip().split()
                    sample_counts[ref] = int(count)

        pandas.DataFrame(merged_counts) \
            .to_csv(get_file_name(output), sep='\t')

rule mapped_read_counts:
    """ 
    Count number of reads mapped to each contig from a sample. 
    
    This just pulls out the refernce column (3) from the SAM output
    and uses uniq -c to get a count.
    """
    input: "{root}.bam.bai"
    output: "{root}.read_counts"
    benchmark: "benchmarks/{root}.read_counts.time"
    params:
        input="{root}.bam"
    shell:
        "samtools view {params.input} \
         | cut -f 3 \
         | uniq -c \
         > {output}"

rule mapped_read_depths:
    """
    Get read depth at each base in each contig. This is needed for
    coverage calculation later.
    """
    input: "{root}.bam.bai"
    output: "{root}.depths"
    benchmark: "benchmarks/{root}.depths.time"
    params:
        input="{root}.bam"
    shell:
        "samtools depth {params.input} > {output}"

rule index_bam:
    input: "{root}.bam"
    output: "{root}.bam.bai"
    benchmark: "benchmarks/{root}.bwa_index_bam.time"
    shell:
        "samtools index {input}"

rule sam_to_bam:
    """ convert to BAM and:
     * sort by contig
     * keep only the primary alignments
    """
    input: "{root}.sam"
    output: "{root}.bam"
    benchmark: "benchmarks/{root}.bwa_sam_bam.time"
    threads: config.get('samtools',{'threads':5})['threads']
    params:
        ram=config.get('samtools',{'ram':'10G'})['ram']
    shell:
        "samtools view -F 2304 -b {input} \
        | samtools sort -l 5 -@ {threads} -m {params.ram} - \
        > {output}"

def pairing_for_sample(wildcards):
    """ return appropriate pairing flag for wildcards.sample """
    reads_file = config['sample_data'][wildcards.sample]
    if isinstance(reads_file, str) or len(reads_file)==1:
        return "-p"
    else:
        return ""

rule search_reads:
    input: 
        reads=lambda w: config['sample_data'][w.sample],
        db='references.bwadb.bwt'
    output: temp(search_result_template)
    threads: 10
    benchmark: "benchmarks/{sample}.bwa.mem.time"
    log: "logs/{sample}.bwa.mem.log"
    params:
        db='references.bwadb',
        pairing=pairing_for_sample
    shell: "bwa mem {params.pairing} -t {threads} {params.db} {input.reads} \
            > {output} 2> {log}"

rule index_refereces:
    """ prepare refereces for mapping with BWA """
    input: references_file
    output: 'references.bwadb.bwt'
    log: "logs/bwa_index.log"
    benchmark: "benchmarks/bwa_pre_index.time"
    params:
        db='references.bwadb'
    shell: "bwa index -p {params.db} {input} > {log} 2>&1 "
