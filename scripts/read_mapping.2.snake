"""
Calculate abundances by mapping reads to a reference catalog

 * Map reads to catalog with BWA or similar
 * count abundances

There are two key configuration values:

  references_file: location of the reference database
  sample_glob: snakemake wildcard glob string to locate read files and name samples

The sample glob should look like:

    /path/to/read/files/{sample}.reads.fastq

Or even:

    SFTP://server.provider.com/remote_path/{project}/{sample}/reads.fastq

Just make sure that one of the wildcards is called 'sample'.


Files (including the glob) can be local or remote. Remote files should be formatted for the snakemake remote provider. EG

   SFTP://server.domain.edu/path/to/file.ext

Supported protocols are SFTP, HTTP, and FTP

login credentials should be passed in the configuration EG:

config = {'remote': {'FTP': {'server.domain.edu': {
    'username': 
    'password':
}

SFTP only works with keys (no password).

"""

# get remote file tools
import os, sys
workflows_dir = os.path.dirname(os.path.dirname(workflow.snakefile))
sys.path.append(workflows_dir)

include "../common/download.snake"
from python.download import remote_wrapper

def check_sample_data(config):
    """ if sample_data not explicitly listed in config, work from glob """
    if "sample_data" not in config:
        if "sample_glob" not in config:
            raise Exception("Please supply read files explicitly in "
                            "config[sample_data] or with config[sample_glob]")
        # string like "/path/to/files/{sample}.fastq"
        sample_glob = config['sample_glob']
        wildcard_values = remote_wrapper(sample_glob, glob=True)
        sample_data = config.setdefault('sample_data', {})
        # which wildcard in glob was "sample"
        wildcard_names = list(wildcard_values._fields)
        sample_index = wildcard_names.index('sample')
        # build sample_data dictionary
        for wildcard_tuple in zip(*list(wildcard_values)):
            for value in wildcard_tuple:
                # skip if wildcard spans folders
                if re.search(os.path.sep, value):
                    break
            else:
                # add sample
                sample = wildcard_tuple[sample_index]
                sample_data[sample] = remote_wrapper(
                                        sample_glob.format(
                                            **dict(zip(wildcard_names,
                                                       wildcard_tuple))))

references_file = remote_wrapper(config['references_file'])
logger.debug("references_file: " + repr(references_file))

search_result_template = "{sample}.vs.references.sam"
filtered_result_template = "{sample}.vs.references.F0.sam"

check_sample_data(config)

search_results = {s:filtered_result_template.format(sample=s) \
                  for s in config['sample_data']}

rule outputs:
    input:
        "reference.hit_counts.tsv",

rule count_table:
    input: search_results.values()
    output: 'reference.hit_counts.tsv'
    benchmark: "benchmarks/count_table.time"
    log: "logs/count_table.log"
    params:
        hit_tables=' '.join(["=".join(kv) for kv in search_results.items()]),
        cutoff=config.get('cutoff', 0)
    shell: "count_taxa.py {params.hit_tables} -p hitid -a portion -F 0 -f sam \
            -o {output} -c {params.cutoff} -v 22> {log}"

rule mapped_read_counts:
    """ 
    Count number of reads mapped to each contig from a sample. 
    
    This just pulls out the refernce column (3) from the SAM output
    and uses uniq -c to get a count.
    """
    input: "{root}.bam.bai"
    output: "{root}.read_counts"
    benchmark: "benchmarks/{root}.read_counts.time"
    params:
        input="{root}.bam"
    shell:
        "samtools view -F 2304 {params.input} \
         | cut -f 3 \
         | uniq -c \
         > {output}"

rule coverages:
    """ turn mapping depth into coverage values """
    input:
        depths="{root}.depths",
        references=references_file
    output: "{root}.coverages"
    run:

rule mapped_read_depths:
    """
    Get read depth at each base in each contig. This is needed for
    coverage calculation later.
    """
    input: "{root}.bam.bai"
    output: "{root}.depths"
    benchmark: "benchmarks/{root}.depths.time"
    params:
        input="{root}.bam"
    shell:
        "samtools depth {params.input} > {output}"

rule index_bam:
    input: "{root}.bam"
    output: "{root}.bam.bai"
    benchmark: "benchmarks/{root}.bwa_index_bam.time"
    shell:
        "samtools index {input}"

rule sam_to_bam:
    """ convert to BAM and:
     * sort by contig
     * keep only the primary alignments
    """
    input: "{root}.sam"
    output: "{root}.bam"
    benchmark: "benchmarks/{root}.bwa_sam_bam.time"
    threads: config.get('samtools',{'threads':5}['threads']
    params:
        ram=config.get('samtools',{'ram':'10G'}['ram']
    shell:
        "samtools view -F 2304 -b {input} \
        | samtools sort -l 5 -@ {threads} -m {params.ram} - \
        > {output}"

def pairing_for_sample(wildcards):
    """ return appropriate pairing flag for wildcards.sample """
    reads_file = config['sample_data'][wildcards.sample]
    if isinstance(reads_file, str) or len(reads_file)==1:
        return "-p"
    else:
        return ""

rule search_reads:
    input: 
        reads=lambda w: config['sample_data'][w.sample],
        db='references.bwadb.bwt'
    output: temp(search_result_template)
    threads: 10
    benchmark: "benchmarks/{sample}.bwa.mem.time"
    log: "logs/{sample}.bwa.mem.log"
    params:
        db='references.bwadb',
        pairing=pairing_for_sample
    shell: "bwa mem {params.pairing} -t {threads} {params.db} {input.reads} \
            > {output} 2> {log}"

rule index_refereces:
    """ prepare refereces for mapping with BWA """
    input: references_file
    output: 'references.bwadb.bwt'
    log: "logs/bwa_index.log"
    benchmark: "benchmarks/bwa_pre_index.time"
    params:
        db='references.bwadb'
    shell: "bwa index -p {params.db} {input} > {log} 2>&1 "
