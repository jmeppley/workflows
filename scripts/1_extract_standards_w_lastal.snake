"""
1_extract_standards_w_lastal

maps prepared reads (from 0_Process_reads_for_mapping_qual) against a 
lastal database of sequencing standard sequences.

You need:
  1.) a set of cleaned fasta files processed by the 0_ script
  2.) a fasta file of the spiked standard sequences
  3.) a table (tab separated text) of spiked standard quantities. EG:

  s10_hyp1806_v1_969_Frag 700
  s11_tynA_v1_996_Frag    35000
  s13_therm_v1_613_Frag   600
  s14_virB_v1_301_Frag    500
  s15_alpha_v1_1504_Frag  37000
  s1_arsB_v1_1068_frag    200
  s2_huyA_v1_1077_Frag    150
  s3_pdhD_v1_1000_Frag    60000
  s4_mcrB_v1_1028_Frag    1000

NB: The standards' names in the first column must match the sequence IDs in the fasta file.

Uses lastal to find standards in your reads. Compares counts to the spiked quantities to get a scaling factor for normalizing each sample's counts.

"""

import numpy
import glob
import os
import sys
import subprocess
import re
import pandas
import random
import shlex
from Bio import SeqIO

##
# where to find the data
# You can set these to something differnt on the command line by adding, EG:
#   --config spike_amounts_table=spikes.txt input_dir=fasta
input_dir = config.get('input_folder', 'fastq')
spike_amounts_table = config.get('spike_amounts_table', "spike_amounts.tsv")
clean_fasta_suffix = config.get('clean_fasta_suffix', ".cut.sickle.non_rRNA.masked.fasta")
standards_fasta = config.get("standards_fasta",
                             "/slipstream/home/faylward/lysine/HOE_Legacy_II/standards/standard_refs/standards.fasta")

## get the list of factor file names to be produced
# this looks for all the lastout files in the give dir
# and replaces the suffix with "factor"
final_factor_files = \
    [re.sub(r'/sorted_rna/', 
            '/standard_info/',
            re.sub(clean_fasta_suffix,
                   '.non_rRNA.masked.fasta.vs.standards.factor',
                   f)
            ) \
     for f in glob.glob(os.path.join(input_dir, 'sorted_rna', "*" + clean_fasta_suffix))]

if len(final_factor_files) == 0:
    raise Exception("No files found in {} matching {}".format(
        os.path.join(input_dir, 'sorted_rna'),
        "*" + clean_fasta_suffix))

## The first rule is run by default
rule collate_factors:
    """
    Collect all the factors into one table
    """
    input: final_factor_files
    output: input_dir + "/normalization_factors.txt"
    shell: "cat {input} > {output}"

rule filter_hits:
    """
    Only keep hits over 98% identical and only the best scoring hit(s) for each read
    """
    input: "{prefix}.lastout"
    output: "{prefix}.I98.L50.lastout"
    shell: "filter_blast_m8.py -f blast -F 0 -I 98 -L 50 {input} > {output}"

rule count_hits:
    """
    Count hits for each sample
    """
    input: "{path}/{sample}.vs.{db}.I98.L50.lastout"
    output: "{path}/{sample}.vs.{db}.counts"
    wildcard_constraints:
        sample=r'[^/]+'
    shell: "count_taxa.py -c 0 -f blast {wildcards.sample}={input} > {output}"

rule calculate_factors:
    """
    generate a normalization factor for each sample
    """
    input: 
        counts="{path}/{sample}.vs.{db}.counts",
        spiked=spike_amounts_table
    output: "{path}/{sample}.vs.{db}.factor"
    wildcard_constraints:
        sample=r'[^/]+'
    run:
        # load counts of recovered standards from this sample
        count_table = pandas.read_table(input.counts, header=0, index_col=0)
        count_table.columns = ['Counts']

        # load spiked in amounts
        spike_table = pandas.read_table(input.spiked, header=None, index_col=0)
        spike_table.columns = ['Spiked']
        
        # get data as lists in same order
        standard_list = sorted(list(count_table.index))
        counts = [count_table.Counts[s] for s in standard_list]
        spikes = [spike_table.Spiked[s] for s in standard_list]
        
        # calculate the scale factor and save
        scale_factor = get_best_fit(counts, spikes, force_intercept=True)[0]
        with open(output[0], 'w') as OUT:
            OUT.write("{0}\t{1:0.2f}\n".format(wildcards.sample, scale_factor))

def get_best_fit(xd, yd, force_intercept=False, force_slope=False):
    """Return coeefs for a line of best fit"""
    #Calculate trendline
    if force_intercept:
        # intercept of 0
        x = numpy.array(xd)[:,numpy.newaxis]
        slope, _, _, _ = numpy.linalg.lstsq(x, yd)
        coeffs = [slope[0], 0]
        if force_slope:
            # We shouldn't get here, but let's just return the fixed values
            coeffs = (1, 0)
    elif force_slope:
        # slope of 1: intercept is average of difference
        intercept = numpy.mean(yd-xd)
        coeffs = [1,intercept]
    else:
        coeffs = numpy.polyfit(xd, yd, 1)

    return coeffs

"""
rule assign_top_hit:
    Resolve cases where one read matches equally well to two standards
    input: all_filtered_hits
    output: all_top_hits
    shell: "assign_top_hit.py ..."
"""

#################################################################################
# Map reads against standard database to count them and remove them before further mapping
################################################################################
standards_root = re.sub(r'\.fasta$','',standards_fasta.strip())
print(standards_root)
standards_prj = standards_root + ".prj"
rule run_lastal:
    input: 
        fasta="{path}/sorted_rna/{prefix}" + clean_fasta_suffix,
        standards=standards_prj
    output: 
        "{path}/standard_info/{prefix}.non_rRNA.masked.fasta.vs.standards.lastout"
    params:
        standards=standards_root
    threads: int(config.setdefault("lastal_threads", 8))
    shell:
        'lastal -P {threads} -Q 0 -f BlastTab {params.standards} {input.fasta} > {output} 2> {output}.log'

rule lastdb:
    input:
        "{db}.fasta"
    output: "{db}.prj"
    shell: "lastdb {wildcards.db} {input}"

