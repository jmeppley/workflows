"""
Called by annotation.gene_catalog.snake to build up the catalog from individual assemblies.

Starting point:
   multiple assembled metagenomes (configured in configfile) 

Workflow:
 * collect all faa and fna annotations
 * pull out just coding sequences from fna files
 * cluster all genes
   - by default use cd-hit-est (nucl) at 95%
   - optins for cd-hit, mmseqs2, vsearch, mcl
 * pull out faa version of cluster reps


Clustering subworkflows:
 * work from all_genes.{faa,ffn} (depending on config[cluster_type]
 * generate all_genes.{faa,ffn}.{cluster_type}
   - eg: all_genes.faa.cdhit
 * this gets linked back to the starting file for annotation
"""

from python.common import parse_stats
from python.gene_catalog import merge_cluster_coverages
from urllib.parse import quote_plus, unquote_plus

#########
# CONFIGURATION
#
# assemblies located in paths
# listed in config[assembly_list] or file config[assembly_list_file]
if "assembly_list" not in config:
    with open(config['assembly_list_file']) as LIST:
        config['assembly_list'] = [l.strip() for l in LIST.readlines()]

# some assemblies need to be renamed to fit in the big catalog
# listed in config as list of 2-entry dicts:
# assembly_renaming:
#   - assembly: /full/path/to/assembly
#     regex: "s/old_prefix/new_prefix/"
#   - assembly: . . .
# Change to a simple dict for use in makefile
assembly_renaming_map = {x['assembly']:x['regex'] \
                         for x in config.get('assembly_renaming', [])}
# chose clustering method
clustering_method = config.get('clustering_method', 'cdhitest')
cluster_suffix = clustering_method
#  clustering snake file should override the variable cluster_suffix
# are we clustering faa or ffn? The cluster method should determine this
cluster_type = config.get("cluster_type", 'ffn')
include: "gene_catalog.{}.snake".format(clustering_method)
#
# Define some trasitions to break the workflow up into segments
transitions = config.setdefault('transitions',{})
# the first part creates the clustered faa (prefix defined in parent makefile)
transitions[prefix + '.faa'] = \
            'all_genes.{}.{}.faa'.format(cluster_type, cluster_suffix)
# the clustered output has different names from different methods
transitions['all_genes.{0}.{1}.{0}'.format(cluster_type, cluster_suffix)] = \
            CLUSTER_REPS
#
# End configuration
##########

##########
# set up list of files to create
#  - The final tabulations
#  - Stats and histrogram files for fasta files
#  - The final annotation table
catalog_stats_files = \
                expand("stats/{prefix}.{suffix}.{ext}", 
                       ext=['stats','hist'],
                       suffix=['faa','ffn'],
                       prefix="all_genes")
config['outputs'] = set(catalog_stats_files)
#
#########

#########
# Helper functions
def get_unique_local_file(file_name, local_dir):
    " return a unique local file name and set up transition "
    name_string = os.path.basename(file_name)
    config['file_count'][name_string] = config.setdefault('file_count', {})\
                                                .get(name_string, 0) + 1
    local_name = local_dir + "/sample_" + \
        str(config['file_count'][name_string]) + "_" + \
        name_string

    config['transitions'][local_name] = file_name
    return local_name
                                                   
def get_all_gene_call_files(ext, config,
                            local_dir='gene_files'):
    """
    Find the contig.annotations.{wildcards.ext} for each assembly

    Replace file name with renamed file name if assembly needs 
    renaming

    Defaults to using unfiltered gene calls

    Uses transitions{} to make a link from the source file to a local alias
    """
    # hack for sliding in fixed gene calls
    path_prefix = config.setdefault('gene_path_prefix', '')
    gene_file_root = config.get('gene_file_root', 'contigs.all.annotations')
    if 'gene_path_prefix' in config and ext=='fna':
        ext='ffn'

    for assembly_path in config['assembly_list']:
        file_name = os.path.join(path_prefix + assembly_path, 
                                 '{}.{}'.format(gene_file_root, ext))

        if assembly_path in assembly_renaming_map:
            file_name = "renamed_dir" + file_name

        yield (quote_plus(assembly_path), 
               get_unique_local_file(file_name, local_dir))


def get_assembly_coverage_files(wildcards):
    """
    Find all the contig.coverages.{sample}.txt for an assembly
    """
    assembly_path = unquote_plus(wildcards.assembly)
    # find a coverage file
    # Try a combined coverage file
    master_file = os.path.join(assembly_path,
                            'contigs.final.stats.txt')
    if os.path.exists(master_file):
        if assembly_path in assembly_renaming_map:
            master_file = "renamed_dir" + master_file
        yield master_file
    else:
        # look for per-sample coverage files
        for file_name in glob.glob(os.path.join(assembly_path,
                                'contigs.all.coverages.*.t??')):
            if assembly_path in assembly_renaming_map:
                file_name = "renamed_dir" + file_name

            yield file_name

def get_assembly_read_stats(wildcards):
    """
    Find all the cleaned reads stats files for an assembly
    """
    assembly_path = unquote_plus(wildcards.assembly)
    for file_name in glob.glob(os.path.join(assembly_path,
                            'stats',
                            '*.clean.fastq.stats')):
        yield file_name


## Collect file names now, so we can set up transitions for remote files
collect_exts = ['faa', 'ffn']
gene_call_files = {e:dict(get_all_gene_call_files(e, config)) for e in collect_exts}

# TODO: as of now, the workflow cant get coverage info from remote servers, so
# the gene counting part of the workflow will fail.

##########
# RULES:
#  This lays out the dependencies and logic of the workflow

## First we collect the genes and cluster them

rule rename_contigs_and_genes:
    """ Renames contigs and genes in any annotation file that needs it """
    input: "{assembly}/{annotation_file}.{ext}"
    output: temp("renamed_dir{assembly}/{annotation_file}.{ext}")
    benchmark: 'benchmarks/rename_annotations{assembly}/{annotation_file}.{ext}.time'
    wildcard_constraints:
        ext='(faa|fna|ffn|txt)'
    params:
        regex=lambda w: assembly_renaming_map[re.sub('^' + \
                            config['gene_path_prefix'], '', \
                            w.assembly)]
    shell: "perl -pe '{params.regex}' \
            < {input} \
            > {output}"

rule collect_gene_calls:
    input: lambda w: gene_call_files[w.ext].values()
    output: "all_genes.{ext}"
    benchmark: 'benchmarks/all_genes.{ext}.time'
    wildcard_constraints:
        ext=r'(ffn|faa)'
    shell:
        """
        cat {input} > {output}
        """

rule get_fasta_id_list:
    input: "{file_root}.{ext}"
    output: "{file_root}.{ext}.idlist"
    benchmark: 'benchmarks/{file_root}.{ext}.idlist.time'
    wildcard_constraints:
        ext=r'(fna|ffn|faa|fasta|fa)'
    shell:
        r"""
        grep "^>" {input} \
         | perl -pe 's/>(\S+)(\s.+)?$`/\1/' \
         | sort \
         | uniq \
         > {output}
        """

# bild reps fasta file for the encoding we did not cluster on
if cluster_type == "ffn":
    rule get_cluster_rep_faa:
        input:
            "all_genes.ffn.{cluster_suffix}.ffn.idlist",
            "all_genes.faa"
        output:
            "all_genes.ffn.{cluster_suffix}.faa"
        benchmark:
            "benchmarks/all_genes.ffn.{cluster_suffix}.faa.time"
        shell: "screen_list.py -l {input[0]} -k {input[1]} > {output}"
elif cluster_type == "faa":
    rule get_cluster_rep_ffn:
        input:
            "all_genes.faa.{cluster_suffix}.faa.idlist",
            "all_genes.ffn"
        output:
            "all_genes.faa.{cluster_suffix}.ffn"
        benchmark:
            "benchmarks/all_genes.faa.{cluster_suffix}.ffn.time"
        shell: "screen_list.py -l {input[0]} -k {input[1]} > {output}"

## We also need to collect the contig coverages
rule assembly_gene_coverages:
    """
    For each assembly:
        map contig coverage onto individual genes
        normalize by total raeds in assembly
    """
    input:
        contig_covs=get_assembly_coverage_files,
        read_stats=get_assembly_read_stats,
        genes=lambda w: gene_call_files['faa'][w.assembly],
    output:
        table=temp('coverages/{assembly}.coverages.tsv'),
    benchmark: 'benchmarks/{assembly}.coverages.time'
    run:
        # get number of reads in assembly
        read_counts = 0
        for stats_file in input.read_stats:
            reads = parse_stats(stats_file)['reads'] / 10000000
            read_counts += reads

        # get dict of contig coverages
        coverages = None
        if len(input.contig_covs) == 0:
            raise Exception("Cannot find coverage file for " +
                            assembly_path)
        contig_col = config.get('contig_col', 'Contig')
        cov_col = config.get('cov_col', 'MeanCov')
        for cov_file in input.contig_covs:
            _coverages = pandas.read_csv(cov_file,
                                         sep='\t',
                                         index_col=0,
                                         header=0,
                                         usecols=[contig_col, cov_col],
                                        )[cov_col] / read_counts
            if coverages is None:
                coverages = _coverages
            else:
                coverages = coverages + _coverages

        # transfer contig coverages to genes
        contig_rexp = re.compile(r'_\d+$')
        def get_gene_cov(gene):
            " get contig name from gene and look up cov "
            return coverages.get(contig_rexp.sub('', gene), 0)
        gene_coverages = \
            pandas.DataFrame(
                {gene.id:{'Coverage':get_gene_cov(gene.id)} \
                          for gene in SeqIO.parse(input.genes, 'fasta')}
            ).T
        gene_coverages.to_csv(output.table)

rule gene_coverage_table:
    """
    collect assembly gene coverages
    use clusters to combine into one big coverage table for clusters
    """
    input: 
        gene_covs=expand('coverages/{assembly}.coverages.tsv',
                         assembly=gene_call_files['faa']),
        clusters=prefix + ".clusters"
    output:
        table='{}.sample.coverage.tsv'.format(prefix),
        sums=temp('{}.coverage.tsv'.format(prefix))
    benchmark: 'benchmarks/{}.sample.coverage.tsv.time'.format(prefix)
    run:
        gene_cov_dict = {a:'coverages/{}.coverages.tsv'.format(a) \
                         for a in gene_call_files['faa']}
        coverages = merge_cluster_coverages(input.clusters,
                                            gene_cov_dict)

        coverages.to_csv(output.table)
        coverages.sum(axis=1).to_csv(output.sums)
 
