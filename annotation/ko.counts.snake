"""
Workflow to compile hit tables from multiple samples into a set of count tables

 * hit id counts by sample (with a column for ko)
"""
import re
import glob
import pandas
import os
import sys
import edl.taxon

snakefile_path=os.path.dirname(os.path.abspath(workflow.main_snakefile))
sys.path[0]=os.path.join(snakefile_path,'..','python')
from common import get_version, parse_stats

if 'ko_map' not in config:
	raise Exception("You must provide a path to the KEGG hit id to KO map as --config ko_map=XXX")

if 'collections' not in config:
    # set up default collections hash with a single collection
    config.setdefault('collections',{})[config.get('collection',
                                                   'compiled')]={}

for collection in config['collections']:
    for collection in config['collections']:
        # build map from stats files to sample names using supplied 
        # glob (to find files) and reg exp (to pull out sample name)
        read_stats_samples = {}
        file_glob = config['collections'][collection].get('read_stats_glob',
                              '*/*_reads.clipped.joined.trimmed.non-rRNA.genes. faa.stats')
        file_re = config['collections'][collection].get('read_stats_re',
                                              '^([^/]+)/')
        for stats_file in glob.glob(file_glob):
            sample = re.search(file_re, stats_file).group(1)
            read_stats_samples[stats_file]=sample
        
        config['collections'][collection]['read_stats_samples'] = \
                                                    read_stats_samples
        
        # build map from hit_tables to sample names using
        # supplied glob and re
        hit_table_samples = {}
        file_glob = config['collections'][collection].get('hit_table_glob',
              '*/*_reads.clipped.joined.trimmed.non-rRNA.genes.  vs.KEGG.lastp')
        file_re = config['collections'][collection].get('hit_table_re',
                                              '^([^/]+)/')
        for hit_table in glob.glob(file_glob):
            sample = re.search(file_re, hit_table).group(1)
            hit_table_samples[sample]=hit_table
    
        config['collections'][collection]['hit_table_samples'] = \
                                                    hit_table_samples

rule all:
    input:
        expand("{collection}/kegg.counts.genes.tsv",
                collection=config['collections']),
        expand("{collection}/kegg.counts.hitids.kos.tsv",
                collection=config['collections']),

rule remove_comments:
    """
    Remove comment lines (they mess up the redistribute algorithm)
    """
    input:
        lambda w: config['collections'][w.collection]['hit_table_samples'][w.sample]
    output:
        temp("{collection}/{sample}.vs.RefSeq.nocomments")
    shell:
        "grep -v '^#' {input} > {output}"

rule hitid_count_table:
    """
    consolodate hit tables into count table by hit id
    """
    input: 
        last=lambda w: expand("{collection}/{sample}.vs.RefSeq.nocomments",
                        collection=w.collection,
                        sample=list(config['collections'][w.collection]\
                                     ['hit_table_samples'].keys())),
        ko_map=config["ko_map"]
    output:
        "{collection}/kegg.counts.hitids.kos.tsv"
    log:
        "{collection}/kegg.counts.hitids.kos.tsv.log"
    params:
        # count_taxa lets us put column names before input files,
        #  so we use this mapping instead of the input list
        input_map=lambda w: " ".join(\
                    ["{0}={1}/{0}.vs.RefSeq.nocomments"\
                                        .format(s,w.collection) \
                      for s in config['collections']\
                                     [w.collection]\
                                     ['hit_table_samples'].keys()])
    shell:
        "count_paths.py {params.input_map} -p hitid -c 0 -a all -C tophit -F 0 -f blast | translate_column.py -m {input.ko_map} -c 1 -f ko > {output} 2> {log}"

rule raw_read_counts:
    """
    loop over read ststs files and generate table of read counts by sample
    """
    input: 
        lambda w: list(config['collections'][w.collection]['read_stats_samples'].keys())
    output:
        "{collection}/kegg.counts.genes.tsv"
    run:
        with open(output[0], 'w') as OUT:
            OUT.write("sample\tread count\n")
            for stats_file in input:
                stats = parse_stats(stats_file)
                sample = config['collections']\
                               [wildcards.collection]\
                               ['read_stats_samples']\
                               [stats_file]
                OUT.write("{}\t{}\n".format(sample,stats['reads']))
                

