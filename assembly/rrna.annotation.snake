include: "../annotation/cmsearch.snake"
include: "../annotation/bwa.snake"

from python.common import get_file_name

# available rRNA DBs
from python.annotate import get_last_alg

blank_file_name = ".empty.file"
rna_search_hits_template = \
    "contigs.vs.rRNA.cmsearch.{rmol}.gt{length}.gff.vs.{db_dot_fmt}"

def get_rna_search_hits(w):
    """ return search result that matches rmol (SSU or LSU) """
    for db, db_data in config.get('dbs',{}).items():
        if db_data.get('type') == 'rrna':
            if re.search(w.rmol, db):
                fmt = db_data.get('format', 'lastdb')
                fmt = get_last_alg(fmt, 'fna')
                db_dot_fmt = "{}.{}".format(db, fmt)
                hits_file = rna_search_hits_template \
                                            .format(db_dot_fmt=db_dot_fmt,
                                                    length=w.length,
                                                    rmol=w.rmol)
                logger.debug ("returning " + hits_file)
                return hits_file
    logger.debug ("returing blank file")
    return blank_file_name

def get_rna_id_names_file(w):
    """ get the .ids file from the Silva db 
        # if rRNA DBs configured, this is a map to id/desc dict
        # otherwise put blank place holder """
    for db, db_data in config.get('dbs',{}).items():
        if db_data.get('type') == 'rrna':
            if re.search(w.rmol, db):
                logger.debug ("returning " + db_data['path'] + ".ids")
                return db_data['path'] + ".ids"
    logger.debug ("returing blank file")
    return blank_file_name

sample_coverages = \
        {s:"contigs.coverages.{sample}.txt".format(sample=s) \
                     for s in config['sample_data'] \
                     if 'clean' in config['sample_data'][s]}

logger.debug("SAMPLE COVERAGES:\n" + repr(sample_coverages))

wildcard_constraints:
    rmol=r'[LS]SU',
    length=r'\d+',

rule long_rrna_gff:
    """ find long LSU or SSU annotations """
    input:
        "{prefix}.gff"
    output:
        "{prefix}.{rmol}.gt{length}.gff"
    benchmark:
        "benchmarks/{prefix}.{rmol}.gt{length}.time"
    version:
        get_version("filter_blast_m8.py")
    shell:
        "grep {wildcards.rmol} {input} \
          | sort \
          | filter_blast_m8.py -s score -L {wildcards.length} -f gff \
            --nonoverlapping \
          > {output}"

rule rrna_fasta:
    """ extract rRNA fasta """
    input:
        fasta="{root}.fasta",
        gff="{root}.vs.{search}.gff"
    output:
        "{root}.vs.{search}.gff.fasta"
    benchmark:
        "benchmarks/{root}.vs.{search}.gff.fasta.time"
    version:
        get_version("get_sequences_from_m8.py")
    shell:
        "cat {input.fasta} \
            | get_sequences_from_m8.py -f gff {input.gff} \
            > {output}"

rule dummy_file:
    """ blank file """
    output: blank_file_name
    shell: "touch {output}"

rule rrna_report:
    """
    Look at the rRNA annotations and get table of nearly full length LSUs or SSUs
    """
    input:
        gff="contigs.vs.rRNA.cmsearch.{rmol}.gt{length}.gff",
        stats=sample_coverages.values(),
        hits=get_rna_search_hits,
        id_names=get_rna_id_names_file,
    output:
        "contigs.vs.rRNA.cmsearch.{rmol}.gt{length}.tsv"
    run:
        # get map from contigs to coverage
        coverages = {}
        for sample, stats_file in sample_coverages.items():
            cov_table = pandas.read_table(stats_file,
                                          index_col=0,
                                          header=0,
                                         )
            coverages[sample] = cov_table[config['cov_col']]

        # table of rRNA model hits from filtered GFF
        features = pandas.read_table(get_file_name(input.gff),
                                     header=None,
                                     names=['contig',
                                            'tool',
                                            'type',
                                            'start',
                                            'end',
                                            'score',
                                            'notes'],
                                     usecols=[0,1,2,3,4,5,8])

        # add coverages for each sample
        coverage_col_names = []
        for sample, sample_covs in coverages.items():
            col_name = 'cov_{}'.format(sample)
            coverage_col_names.append(col_name)
            features[col_name] = \
                [sample_covs[features.loc[i,'contig']]\
                 for i in features.index]

        # index on contig, start, and end, and just take a few columns
        output_index_columns = ['contig', 'start', 'end']
        output_columns = coverage_col_names + ['type', 'score']
        output_table = features.set_index(output_index_columns)[output_columns]

        # get Silva annotation(s) for each feature
        from edl.blastm8 import generate_hits
        from edl.util import parseMapFile
        
        m = re.search(r'gff\.vs\.(.+)', get_file_name(input.hits))
        if m is not None:
            hit_table_name = re.search(r'gff\.vs\.(.+)', input.hits).group(1)
            # should be, eg, SilvaSSU.lastn or SilvaSSU.bwa.sam
            words = hit_table_name.split('.')
            # first word is DB name
            db = words[0]
            # last word is fmt for parsing
            hit_format = words[-1]
            if hit_format.startswith('last'):
                hit_format = 'last'
            elif hit_format.startswith('blast'):
                hit_format = 'blastplus'
            hit_descriptions = parseMapFile(input.id_names)
            for gene, hits in generate_hits(get_file_name(input.hits),
                                            format=hit_format):
                # take the first hit
                hit = list(hits)[0]
                contig, start, end = re.search(r'^(.+)_(\d+)_(\d+)$', gene)\
                                                                        .groups()
                index = (contig, int(start), int(end))
                if index not in output_table.index:
                    index = (contig, int(end), int(start))
                desc = hit_descriptions[hit.hit]
                output_table.loc[index, db] = desc

        output_table.to_csv(output[0], sep='\t')
